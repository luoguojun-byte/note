```
yum repolist
yum install centos-release-openstack-newton


pipeline = healthcheck cors sizelimit http_proxy_to_wsgi osprofiler url_normalize request_id build_auth_context token_auth json_body ec2_extension public_service

pipeline = healthcheck cors sizelimit http_proxy_to_wsgi osprofiler url_normalize request_id build_auth_context token_auth json_body ec2_extension s3_extension admin_service 
vi /etc/yum.repos.d/OpenStack-Newton.repo
[OpenStack-Newtron]
name=OpenStack-Newtron
baseurl=http://vault.centos.org/7.2.1511/cloud/x86_64/openstack-newton/
gpgcheck=0
enabled=1

1．根据表1中的IP地址规划，设置各服务器节点的IP地址，确保网络正常通信，设置云服务器1主机名为Controller，云服务器2主机名为Compute，并在各服务器节点中设置主机名与IP地址的映射、关闭防火墙并设置为开机不启动、SELinux 为 Permissive 模式。（1分）

controller
##地址映射
hostnamectl set-hostname controller
vi /etc/hosts
172.129.29.10 controller
172.129.29.20 compute
##防火墙
iptables  -F
iptables  -X
iptables  -Z
systemctl disable firewall
##Selinux
vi /etc/selinux/conf
permissive

compute
##地址映射
hostnamectl set-hostname compute
vi /etc/hosts
172.129.29.10 controller
172.129.29.20 compute
##防火墙
iptables  -F
iptables  -X
iptables  -Z
systemctl disable firewall
##Selinux
vi /etc/selinux/conf
permissive

2．将提供的CentOS-7-x86_64-DVD-1804.iso和OpenStackQueens.iso光盘镜像上传到Controller服务器的/root目录下，然后在/opt目录下分别创建centos目录和openstack目录，并将镜像文件CentOS-7-x86_64-DVD-1804.iso挂载到centos目录下，将镜像文件OpenStackQueens.iso挂载到openstack目录下。（1分）

controller
mkdir /opt/centos
mkdir /opt/openstack
mount /Cxxx  /opt/centos
mount /oxxx /opt/openstack
3．在Controller服务器上利用centos目录中的软件包安装vsftp服务器并设置开机自启动，提供yum仓库服务，并分别设置controller和compute服务器的yum源文件ftp.repo，其中节点的地址使用主机名形式。（1分）

controller 
rm -rf /etc/yum.repo/*
yum clean all
vi /etc/yum.repos.d/ftp.repo 
[centos]
name=centos
baseurl=file:///opt/centos
gpgcheck=0
enabled=1

[openstack]
name=openstack
baseurl=file:///opt/openstack/
gpgcheck=0
enabled=1

yum -y install vsftpd
vi /etc/vsftpd/vsftpd.conf
anon_root=/opt
systemctl restart vsftpd
systemctl enable vsftpd


compute
[centos]
name=centos
baseurl=ftp://controller/centos
gpgcheck=0
enabled=1

[openstack]
name=openstack
baseurl=ftp://controller/openstack/
gpgcheck=0
enabled=1

4．在Controller服务器上部署chrony服务器，允许其他节点同步时间，启动服务并设置为开机启动；并在compute服务器上指定controller服务器为上游NTP服务器，重启服务并设为开机启动。（1分）

controller
vi /etc/chrony.conf
allow 172.129.29.10/24
local stratum 10
systemctl restart chronyd
systemctl enable chronyd
chronyc sources
compute
vi /etc/chrony.conf
server 172.129.29.10 ibursr #其他的注释
systemctl restart chronyd
systemctl enable chronyd
chronyc sources
5．在compute节点上创建2个100G的磁盘分区。（1分）
compute  
fdisk -l
fdisk /dev/xxx 
n->p->1->回车->回车->w ## n：添加一个分区 P：主分区 两个回车指是开始和结束的磁盘扇区大小 w：写入磁盘

1．在controller和compute节点上分别安装Openstack client。（0.2分）
controller 
yum -y install  python-openstackclient
compute
yum -y install  python-openstackclient

2．在controller节点上安装mariadb服务并完成配置。（0.2分）
controller
yum install mariadb mariadb-server python2-PyMySQL  -y
vi /etc/my.cnf.d/mariadb-server.cnf
bind-address = 0.0.0.0
chown mysql:mysql -R /var/lib/mysql/
systemctl enable mariadb.service
systemctl start mariadb.service
#账号初始化
  mysql_secure_installation
#远程访问设置（用于后期其他节点连接）
  GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '000000';

3．在controller节点上安装Message queue服务，并完成“openstack”用户创建、授权和角色设置。（0.5分）
controller
yum install rabbitmq-server -y
systemctl start rabbitmq-server
systemctl enable rabbitmq-server

rabbitmqctl add_user openstack 000000 #用户创建
rabbitmqctl set_permissions openstack ".*" ".*" ".*"  #授权
rabbitmqctl set_user_tags openstack administrator #角色设置
4．在controller节点上安装Memcached服务并完成配置（0.2分）
controller
yum -y install memcached python-meemcached
vi /etc/sysconfig/memcached 
#主要增加controller
OPTIONS="-l 127.0.0.1,::1,controller"

systemctl start mecached
systemctl enable memcached
5．在controller节点上安装etcd服务并完成配置（0.4分）
controller
yum -y install etcd

ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_PEER_URLS="http://172.129.29.10:2380"
ETCD_LISTEN_CLIENT_URLS="http://172.129.29.10:2379"
ETCD_NAME="controller"
ETCD_ADVERTISE_CLIENT_URLS="http://172.129.29.10:2379"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://172.129.29.10:2380"
ETCD_INITIAL_CLUSTER="controller=http://172.129.29.10:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster-01"
ETCD_INITIAL_CLUSTER_STATE="new"

6．在controller节点上创建并设置keystone数据库、创建相关域/项目/用户/角色等步骤，完成keystone认证服务的安装和配置（1分）
controller
mysql -u root -p
数据库密码
CREATE DATABASE keystone;
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' IDENTIFIED BY '000000';
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' IDENTIFIED BY '000000';

yum -y install openstack-keystone httpd mod_wsgi 
vi /etc/keystone/keystone.conf
#数据库连接
connection = mysql+pymysql://keystone:000000@controller/keystone
provider = fernet
driver=memcached
#同步keystone数据库
su -s /bin/sh -c "keystone-manage db_sync" keystone
#初始化Fernet密钥存储库
keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone
keystone-manage credential_setup --keystone-user keystone --keystone-group keystone

keystone-manage bootstrap --bootstrap-password 000000 \
--bootstrap-admin-url http://controller:5000/v3/ \
--bootstrap-internal-url http://controller:5000/v3/ \
--bootstrap-public-url http://controller:5000/v3/ \
--bootstrap-region-id RegionOne

#配置Apache HTTP服务
vi /etc/httpd/conf/httpd.conf
ServerName controller
#添加链接
ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/
#完成安装重启
systemctl enable httpd.service
systemctl start httpd.service
#配置admin用户的环境变量
export OS_USERNAME=admin
export OS_PASSWORD=000000
export OS_PROJECT_NAME=admin
export OS_USER_DOMAIN_NAME=default
export OS_PROJECT_DOMAIN_NAME=default
export OS_AUTH_URL=http://controller:35357/v3
export OS_IDENTITY_API_VERSION=3

#创建域
openstack domain create --description "demo" example
#创建服务项目
openstack project create --domain default --description "Service Project" service
#创建Demo Project
openstack project create --domain demo --description "Demo Project" demo
#创建keystone 用户
openstack user create --domain demo --password demo DOMAIN_NAME
#创建角色并且将角色添加到demo项目和用户 
openstack role create keystone
openstack role add --project demo --user DOMAIN_NAME user


7．在controller节点上创建并设置glance数据库、创建glance镜像服务的keystone相关认证信息等步骤，完成glance镜像服务的安装和配置（1分）。
controller
mysql -u root -p
CREATE DATABASE glance;
GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' IDENTIFIED BY '000000';
GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' IDENTIFIED BY '000000';

#源的admin凭证获取admin-only CLI 命令
vi /root/admin-openrc
export OS_PROJECT_DOMAIN_NAME=Default
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=123456
export OS_AUTH_URL=http://controller:5000/v3
export OS_IDENTITY_API_VERSION=3
export OS_IMAGE_API_VERSION=2
. admin-openrc
#创建glance用户
openstack user create --domain demo --password 000000 glance
#添加admin角色glance用户和service项目
openstack role add --project service --user glance admin
#创建glance服务单位
openstack service create --name glance --description "OpenStack Image" image
#创建图像服务API端点
openstack endpoint create --region RegionOne image public http://controller:9292
openstack endpoint create --region RegionOne image internal http://controller:9292
openstack endpoint create --region RegionOne image admin http://controller:9292
#安装
yum install openstack-glance

#正则表达式grep过滤
过滤#空格后面的和空行和以#结尾的
greo -Ev "# |^$|#$" glance-api.conf > glance-api.conf
#编辑配置文件
vi  /etc/glance/glance-api.conf 
[database]
#...
connection = mysql+pymysql://glance:000000@controller/glance

[keystone_authtoken]
# ...
auth_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller:11211
auth_type = password
project_domain_name = demo
user_domain_name = demo
project_name = service
username = glance
password = 000000

[paste_deploy]
# ...
flavor = keystone

[glance_store]
# ...
stores = file,http
default_store = file
filesystem_store_datadir = /var/lib/glance/images/

[database]
# ...
connection = mysql+pymysql://glance:000000@controller/glance

[keystone_authtoken]
# ...
auth_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller:11211
auth_type = password
project_domain_name = demo
user_domain_name = demo
project_name = service
username = glance
password = GLANCE_PASS

[paste_deploy]
# ...
flavor = keystone

#填充图形服务数据库
su -s /bin/sh -c "glance-manage db_sync" glance
#启动
systemctl enable openstack-glance-api.service openstack-glance-registry.service
systemctl start openstack-glance-api.service openstack-glance-registry.service

8．在controller节点上创建并设置nova数据库、创建nova计算服务的keystone相关认证信息、创建placement服务的keystone相关认证信息等步骤，完成nova计算服务的安装和配置（1分）。
controller
mysql -u root -p
CREATE DATABASE nova;
CREATE DATABASE nova_api;
CREATE DATABASE nova_cell0;

GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' IDENTIFIED BY '000000';
GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' IDENTIFIED BY '000000';

GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' IDENTIFIED BY '000000';
GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' IDENTIFIED BY '000000';

GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'localhost' IDENTIFIED BY '000000';
GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'%' IDENTIFIED BY '000000';

#凭证 CLI命令
. admin-openrc
#创建nova用户
openstack user create --domain demo --password 000000 nova
#添加admin角色nova用户
openstack role add --project service --user nova admin
#创建nova服务单位
openstack service create --name nova --description "OpenStack Compute" compute
#创建计算API服务端点
openstack endpoint create --region RegionOne compute public http://controller:8774/v2.1
openstack endpoint create --region RegionOne compute internal http://controller:8774/v2.1
openstack endpoint create --region RegionOne compute admin http://controller:8774/v2.1
#创建placement服务
openstack user create --domain demo --password 000000 placement
#添加用户位置的服务项目管理角色
openstack role add --project service --user placement admin
#创建服务目录中的位置API条目
openstack service create --name placement --description "Placement API" placement
#创建API服务端点位置
openstack endpoint create --region RegionOne placement public http://controller:8778
openstack endpoint create --region RegionOne placement internal http://controller:8778
openstack endpoint create --region RegionOne placement admin http://controller:8778
#安装
yum -y install openstack-nova-api openstack-nova-conductor 
openstack-nova-console openstack-nova-novncproxy 
openstack-nova-scheduler openstack-nova-placement-api
#编辑配置文件
vi  /etc/nova/nova.conf
[DEFAULT]
# ...
enabled_apis = osapi_compute,metadata
transport_url = rabbit://openstack:000000@controller
my_ip = 自己的IP
use_neutron = True
firewall_driver = nova.virt.firewall.NoopFirewallDriver

[api_database]
# ...
connection = mysql+pymysql://nova:000000@controller/nova_api

[database]
# ...
connection = mysql+pymysql://nova:000000@controller/nova

[api]
# ...
auth_strategy = keystone

[keystone_authtoken]
# ...
auth_url = http://controller:5000/v3
memcached_servers = controller:11211
auth_type = password
project_domain_name = demo
user_domain_name = demo
project_name = service
username = nova
password = 000000

[vnc]
enabled = true
# ...
server_listen = $my_ip
server_proxyclient_address = $my_ip

[glance]
# ...
api_servers = http://controller:9292

[oslo_concurrency]
# ...
lock_path = /var/lib/nova/tmp

[placement]
# ...
os_region_name = RegionOne
project_domain_name = Default
project_name = service
auth_type = password
user_domain_name = demo
auth_url = http://controller:5000/v3
username = placement
password = 000000

[scheduler]
discover_hosts_in_cells_interval = 300

vi /etc/httpd/conf.d/00-nova-placement-api.conf
<Directory /usr/bin>
   <IfVersion >= 2.4>
      Require all granted
   </IfVersion>
   <IfVersion < 2.4>
      Order allow,deny
      Allow from all
   </IfVersion>
</Directory>
#重启httpd服务
systemctl restart httpd
#填充nova-api数据库
su -s /bin/sh -c "nova-manage api_db sync" nova
#注册cell
su -s /bin/sh -c "nova-manage cell_v2 map_cell0" nova
#填充新数据库
su -s /bin/sh -c "nova-manage db sync" nova
#验证cell0和cell1正确注册
nova-manage cell_v2 list_cells
#重启服务
systemctl enable openstack-nova-api.service 
openstack-nova-consoleauth.service openstack-nova-scheduler.service 
openstack-nova-conductor.service openstack-nova-novncproxy.service
systemctl start openstack-nova-api.service 
openstack-nova-consoleauth.service openstack-nova-scheduler.service 
openstack-nova-conductor.service openstack-nova-novncproxy.service
9．在compute节点上完成nova计算服务的安装和配置（1分）。
#安装
yum install openstack-nova-compute
#配置文件
vi /etc/nova/nova.conf

[DEFAULT]
# ...
enabled_apis = osapi_compute,metadata
transport_url = rabbit://openstack:000000@controller
my_ip = 自己的IP
use_neutron = True
firewall_driver = nova.virt.firewall.NoopFirewallDriver

[api]
# ...
auth_strategy = keystone

[keystone_authtoken]
# ...
auth_url = http://controller:5000/v3
memcached_servers = controller:11211
auth_type = password
project_domain_name = demo
user_domain_name = demo
project_name = service
username = nova
password = 000000

[vnc]
# ...
enabled = True
server_listen = 0.0.0.0
server_proxyclient_address = $my_ip
novncproxy_base_url = http://controller:6080/vnc_auto.html

[glance]
# ...
api_servers = http://controller:9292

[oslo_concurrency]
# ...
lock_path = /var/lib/nova/tmp

[placement]
# ...
os_region_name = RegionOne
project_domain_name = demo
project_name = service
auth_type = password
user_domain_name = demo
auth_url = http://controller:5000/v3
username = placement
password = 000000

[libvirt]
# ...
virt_type = qemu

[scheduler]
discover_hosts_in_cells_interval = 300
#确定计算节点支持硬件加速
egrep -c '(vmx|svm)' /proc/cpuinfo
#启动服务
systemctl enable libvirtd.service openstack-nova-compute.service
systemctl start libvirtd.service openstack-nova-compute.service
#添加计算节点到cell数据库
#凭证
. admin-openrc
#发现计算主机
su -s /bin/sh -c "nova-manage cell_v2 discover_hosts --verbose" nova

10．在controller节点上创建并设置neutron数据库、创建neutron网络服务的keystone相关认证信息等步骤，完成neutron网络服务的安装和配置（1分）。
mysql -u root -p
CREATE DATABASE neutron;
GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' IDENTIFIED BY '000000';
GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' IDENTIFIED BY '000000

#凭证 CLI 命令
.admin-openrc
#创建neutron用户
openstack user create --domain demo --password 000000 neutron
#添加admin角色neutron用户
openstack role add --project service --user neutron admin
#创建neutron服务单位
openstack service create --name neutron --description "openstack networking" network
#创建网络服务API端点
openstack endpoint create --region RegionOne network public http://controller:9696
openstack endpoint create --region RegionOne network internal http://controller:9696
openstack endpoint create --region RegionOne network admin http://controller:9696
#网络安装
yum install openstack-neutron openstack-neutron-ml2 openstack-neutron-linuxbridge ebtables
#编辑配置文件
vi /etc/neutron/neutron.conf
[database]
# ...
connection = mysql+pymysql://neutron:000000@controller/neutron

[DEFAULT]
# ...
core_plugin = ml2
service_plugins = router
allow_overlapping_ips = true
transport_url = rabbit://openstack:000000@controller
auth_strategy = keystone
notify_nova_on_port_status_changes = true
notify_nova_on_port_data_changes = true


[keystone_authtoken]
# ...
auth_uri = http://controller:5000
auth_url = http://controller:35357
memcached_servers = controller:11211
auth_type = password
project_domain_name = demo
user_domain_name = demo
project_name = service
username = neutron
password = 000000

[nova]
# ...
auth_url = http://controller:35357
auth_type = password
project_domain_name = demo
user_domain_name = demo
region_name = RegionOne
project_name = service
username = nova
password = 000000

[oslo_concurrency]
# ...
lock_path = /var/lib/neutron/tmp

#配置模块化层（ML2）
##vi /etc/neutron/plugins/ml2/ml2_conf.ini
[ml2]
# ...
type_drivers = flat,vlan,vxlan
tenant_network_types = vxlan
mechanism_drivers = linuxbridge,l2population
extension_drivers = port_security

[ml2_type_flat]
# ...
flat_networks = provider

[ml2_type_vxlan]
# ...
vni_ranges = 1:1000

[securitygroup]
# ...
enable_ipset = true
#配置Linux代理桥
## vi /etc/neutron/plugins/ml2/linuxbridge_agent.ini
[linux_bridge]
physical_interface_mappings = provider:PROVIDER_INTERFACE_NAME

[vxlan]
enable_vxlan = true
local_ip = OVERLAY_INTERFACE_IP_ADDRESS
l2_population = true

[securitygroup]
# ...
enable_security_group = true
firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver

net.bridge.bridge-nf-call-iptables
net.bridge.bridge-nf-call-ip6tables

#配置第三层代理
##vi /etc/neutron/l3_agent.ini
[DEFAULT]
# ...
interface_driver = linuxbridge
#配置DHCP代理
[DEFAULT]
# ...
interface_driver = linuxbridge
dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq
enable_isolated_metadata = true



#配置元数据代理
vi /etc/neutron/metadata_agent.ini 
[DEFAULT]
# ...
nova_metadata_host = controller
metadata_proxy_shared_secret = METADATA_SECRET

#使用网络服务配置计算服务
vi /etc/nova/nova.conf
[neutron]
# ...
url = http://controller:9696
auth_url = http://controller:35357
auth_type = password
project_domain_name = demo
user_domain_name = demo
region_name = RegionOne
project_name = service
username = neutron
password = 000000
service_metadata_proxy = true
metadata_proxy_shared_secret = METADATA_SECRET

#网络初始化符号链接
ln - s /etc/neutron/plugins/ml2/ml2_conf. ini /etc/neutron/plugin.ini

#填充数据库
su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf \
  --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron
#重启计算API服务
systemctl restart openstack-nova-api.service
#启动
systemctl enable neutron-server.service \
neutron-linuxbridge-agent.service neutron-dhcp-agent.service \
neutron-metadata-agent.service

systemctl start neutron-server.service \
neutron-linuxbridge-agent.service neutron-dhcp-agent.service \
neutron-metadata-agent.service

systemctl enable neutron-l3-agent.service
systemctl start neutron-l3-agent.service
12．在controller节点上完成Dashboard服务的安装和配置（0.5分）。
#安装
yum install openstack-dashboard
#配置
vi /etc/openstack-dashboard/local_settings 
OPENSTACK_HOST = "controller"
ALLOWED_HOSTS = ['*']
#配置memcached会话存储服务
SESSION_ENGINE = 'django.contrib.sessions.backends.cache'

CACHES = {
    'default': {
         'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',
         'LOCATION': 'controller:11211',
    }
}
# 使身份API版本3
OPENSTACK_KEYSTONE_URL = "http://%s:5000/v3" % OPENSTACK_HOST
#支持域
OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT =True
#配置Default作为默认用户
OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = "Default"
#配置user为默认角色，通过仪表盘创建
OPENSTACK_KEYSTONE_DEFAULT_ROLE = "user"
#时区
TIME_ZONE = "TIME_ZONE"

## vi /etc/httpd/conf.d/openstack-dashboard.conf
#添加这行
WSGIApplicationGroup %{GLOBAL}
#完成安装重启
systemctl restart httpd.service memcached.service
13．在controller和compute节点上完成manila服务的安装和配置（2分）。
controller
mysql -u root -p
create database manila;
grant all privileges on manila.* to 'manila'@'localhost' identified by '000000';
grant all privileges on manila.* to 'manila'@'%' identified by '000000';

#创建服务凭证和API端点
. admin-openrc
openstack user create --domain demo --password 000000 manila
openstack role add --project service --user manila admin
openstack service create --name manila --description "OpenStack Shared File Systems" share
openstack service create --name manilav2 --description "OpenStack Shared File Systems" sharev2
openstack endpoint create --region RegionOne share public http://controller:8786
openstack endpoint create --region RegionOne share internal http://controller:8786
openstack endpoint create --region RegionOne share admin http://controller:8786
openstack endpoint create --region RegionOne sharev2 public http://controller:8786
openstack endpoint create --region RegionOne sharev2 internal http://controller:8786
openstack endpoint create --region RegionOne sharev2 admin http://controller:8786

#安装并配置Heat
yum install -y openstack-manila python-manilaclient
#vi /etc/manila/manila.conf
[DEFAULT]
rpc_backend = rabbit
default_share_type = default_share_type
rootwrap_config = /etc/manila/rootwrap.conf
auth_strategy = keystone
my_ip = 自己的IP
    
[database]
connection = mysql+pymysql://manila:000000@controller/manila
    
[oslo_messaging_rabbit]
rabbit_host = controller
rabbit_userid = openstack
rabbit_password = 000000
    
[keystone_authtoken]
memcached_servers = controller:11211
auth_uri = http://controller:5000
auth_url = http://controller:35357
auth_type = password
project_domain_name = demo
user_domain_name = demo
project_name = service
username = manila
password = 000000
    
[oslo_concurrency]
lock_path = /var/lib/manila/tmp
#同步数据库
su -s /bin/sh -c "manila-manage db sync" manila
#启动Manila服务并设置开机自启
systemctl enable openstack-manila-api.service openstack-manila-scheduler.service
systemctl start openstack-manila-api.service openstack-manila-scheduler.service

compute
#安装Manila软件包
yum install -y openstack-manila-share python2-PyMySQL
#修改Manila组件相关配置文件
vi /etc/manila/manila.conf
[DEFAULT]
rpc_backend = rabbit
default_share_type = default_share_type
rootwrap_config = /etc/manila/rootwrap.conf
auth_strategy = keystone
my_ip = 自己的IP
    
[oslo_messaging_rabbit]
rabbit_host = controller
rabbit_userid = openstack
rabbit_password = 000000
    
[keystone_authtoken]
memcached_servers = controller:11211
auth_uri = http://controller:5000
auth_url = http://controller:35357
auth_type = password
project_domain_name = demo
user_domain_name = demo
project_name = service
username = manila
password = 000000
    
[oslo_concurrency]
lock_path = /var/lib/manila/tmp
#安装相关软件包
yum install -y openstack-neutron openstack-neutron-linuxbridge ebtables
#修改配置文件
vi /etc/manila/manila.conf
[DEFAULT]
enabled_share_backends = generic
enabled_share_protocols = NFS,CIFS
    
[generic]
share_backend_name = GENERIC
share_driver = manila.share.drivers.generic.GenericShareDriver
driver_handles_share_servers = True
service_instance_flavor_id = 100
service_image_name = manila-service-image
service_instance_user = manila
service_instance_password = manila
interface_driver = manila.network.linux.interface.BridgeInterfaceDriver
    
[neutron]
url = http://controller:9696
auth_uri = http://controller:5000
auth_url = http://controller:35357
memcached_servers = controller:11211
auth_type = password
project_domain_name = demo
user_domain_name = demo
region_name = RegionOne
project_name = service
username = neutron
password = 000000
    
[nova]
auth_uri = http://controller:5000
auth_url = http://controller:35357
memcached_servers = controller:11211
auth_type = password
project_domain_name = demo
user_domain_name = demo
region_name = RegionOne
project_name = service
username = nova
password = 000000
    
[cinder]
auth_uri = http://controller:5000
auth_url = http://controller:35357
memcached_servers = controller:11211
auth_type = password
project_domain_name = demo
user_domain_name = demo
region_name = RegionOne
project_name = service
username = cinder
password = 000000
#启动服务
systemctl enable openstack-manila-share.service
systemctl start openstack-manila-share.service
```

