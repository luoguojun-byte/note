## 一、基础运维任务

### 基础环境配置

```
controller eth0 1192.168.x.10/24 vlan x
		   eth1 192.168.y.10/24 vlan y
compute    eth0 192.168.x.20/24 vlan x
		   eth1 192.168.y.20/24 vlan y
```

#### 1.修改网卡IP

controller节点

```
ens19: 10.0.0.61
DEVICE=ens19
TYPE=Ethernet
ONBOOT=yes
NM_CONTROLLED=no
BOOTPROTO=static
IPADDR=10.0.0.61
PREFIX=24
GATEWAY=10.0.0.254

eth0: 172.129.29.10
DEVICE=eth0
TYPE=Ethernet
ONBOOT=yes
NM_CONTROLLED=no
BOOTPROTO=static
IPADDR=172.129.29.10
PREFIX=24
```

compute节点

```
ens19: 10.0.0.62
DEVICE=ens19
TYPE=Ethernet
ONBOOT=yes
NM_CONTROLLED=no
BOOTPROTO=static
IPADDR=10.0.0.62
PREFIX=24
GATEWAY=10.0.0.254

eth0: 172.129.29.20
DEVICE=eth0
TYPE=Ethernet
ONBOOT=yes
NM_CONTROLLED=no
BOOTPROTO=static
IPADDR=172.129.29.20
PREFIX=24
```

#### 2.修改主机名

设置控制节点主机名为 controller，设置计算节点主机名为 compute； 

```
hostnamectl set-hostname controller
hostnamectl set-hostname compute
```

#### 3.修改 hosts 文件将 IP 地址映射为主机名；

controller&compute

```
vi /etc/hosts
10.0.0.61 controller
10.0.0.62 compute
```

#### 4.关闭控制节点的防火墙，设置开机不启动

```
systemctl stop firewalld
systemctl disable firewalld
yum remove -y NetworkManager firewalld
yum -y install iptables-services
systemctl enable iptables
systemctl restart iptables
iptables -F
iptables -X
iptables -Z
service iptables save
```

#### 5.设置 SELinux 为 Permissive 模式

```
vi /etc/selinux/config
SELINUX=permissive
```

### 镜像挂载

#### 1.镜像上传

将提供的 CentOS-7-x86_64-DVD-1804.iso和chinaskill_cloud_iaas.iso 光盘镜像上传到 controller 节点/root目录下

#### 2.镜像挂载

```
mkdir /opt/centos
mkdir /opt/centos
mount CentOS-7-x86_64-DVD-1804.iso /opt/centos
mount chinaskill_cloud_iaas.iso /opt/openstack
```

### Yum 源配置

#### 1.配置yum源

controller

```
rm -rf /etc/yum.repo.d/*
vi /etc/yum.repo.d/ftp.repo
[centos]
name=centos
baseurl=file:///opt/centos
gpgcheck=0
enabled=1

[iaas]
name=iaas
baseurl=file:///opt/iaas/iaas-repo
gpgcheck=0
enabled=1
```

compute

```
rm -rf /etc/yum.repo.d/*
scp root@controller:/etc/yum.repo.d/ftp.repo /etc/yum.repo.d/
vi /etc/yum.repo.d/ftp.repo
[centos]
name=centos
baseurl=ftp://controller/centos
gpgcheck=0
enabled=1

[iaas]
name=iaas
baseurl=ftp://controller/iaas/iaas-repo
gpgcheck=0
enabled=1
```

controller&compute

```
yum clean all
yum makecache
```

#### 2.安装vsftpd服务

设置开机自启动，并使用 ftp 提供 yum 仓库服务（ftp共享的目录为/opt）

```
yum install -y vsftpd

vi /etc/vsftpd/vsftpd.con
anon_root=/opt/
```

#### 3.时间同步配置

controller&compute

```
yum install -y chrony
```

controller

```
vi /etc/chrony.conf
添加以下内容（删除默认sever规则）
server controller iburst
allow 192.168.100.0/24
local stratum 10

启动ntp服务器
systemctl restart chronyd
systemctl enable chronyd
```

compute

```
server controller iburst
启动ntp服务器

systemctl restart chronyd
systemctl enable chronyd
```

#### 4.计算节点分区

compute

```
[root@compute ~]# parted /dev/sdb                                     
(parted) mkpart cinder 0G 100G  
(parted) mkpart swfit 1000G 200G 
[root@compute ~]# mkfs.xfs /dev/sdb1
[root@compute ~]# mkfs.xfs /dev/sdb2
```



## 二、OpenStack 搭建任务

### 基础安装

controller和compute节点

```
yum install iaas-xiandian -y
```

### 变量配置

在控制节点和计算节点上分别安装 iaas-xiandian 软件包，根据表 2 配置两个节点脚本文件中的基本变量（配置脚本文件为/etc/xiandian/openrc.sh）

```
[root@controller ~]# sed 's/^#//g' /etc/xiandian/openrc.sh -i
[root@controller ~]# sed 's/PASS=$/PASS=000000/g' /etc/xiandian/openrc.sh 
#Controller Server Manager IP. example:x.x.x.x
HOST_IP=10.0.0.61

#Controller HOST Password. example:000000 
HOST_PASS=000000

#Controller Server hostname. example:controller
HOST_NAME=controller

#Compute Node Manager IP. example:x.x.x.x
HOST_IP_NODE=10.0.0.62

#Compute HOST Password. example:000000 
HOST_PASS_NODE=000000

#Compute Node hostname. example:compute
HOST_NAME_NODE=compute

#--------------------Chrony Config-------------------##
#Controller network segment IP.  example:x.x.0.0/16(x.x.x.0/24)
network_segment_IP=10.0.0.0/24

#--------------------Rabbit Config ------------------##
#user for rabbit. example:openstack
RABBIT_USER=openstack

#Password for rabbit user .example:000000
RABBIT_PASS=000000

#--------------------MySQL Config---------------------##
#Password for MySQL root user . exmaple:000000
DB_PASS=000000

#--------------------Keystone Config------------------##
#Password for Keystore admin user. exmaple:000000
DOMAIN_NAME=demo
ADMIN_PASS=000000
DEMO_PASS=000000

#Password for Mysql keystore user. exmaple:000000
KEYSTONE_DBPASS=000000

#--------------------Glance Config--------------------##
#Password for Mysql glance user. exmaple:000000
GLANCE_DBPASS=000000

#Password for Keystore glance user. exmaple:000000
GLANCE_PASS=000000

#--------------------Nova Config----------------------##
#Password for Mysql nova user. exmaple:000000
NOVA_DBPASS=000000

#Password for Keystore nova user. exmaple:000000
NOVA_PASS=000000

#--------------------Neturon Config-------------------##
#Password for Mysql neutron user. exmaple:000000
NEUTRON_DBPASS=000000

#Password for Keystore neutron user. exmaple:000000
NEUTRON_PASS=000000

#metadata secret for neutron. exmaple:000000
METADATA_SECRET=000000

#Tunnel Network Interface. example:x.x.x.x   ###控制节点IP，在哪个节点就是哪个IP
INTERFACE_IP=10.0.0.61

#External Network Interface. example:eth1
INTERFACE_NAME=eth0

#External Network The Physical Adapter. example:provider
Physical_NAME=provider

#First Vlan ID in VLAN RANGE for VLAN Network. exmaple:101
minvlan=1

#Last Vlan ID in VLAN RANGE for VLAN Network. example:200
maxvlan=200

#--------------------Cinder Config--------------------##
#Password for Mysql cinder user. exmaple:000000
CINDER_DBPASS=000000

#Password for Keystore cinder user. exmaple:000000
CINDER_PASS=000000

#Cinder Block Disk. example:md126p3
BLOCK_DISK=sdb1

#--------------------Swift Config---------------------##
#Password for Keystore swift user. exmaple:000000
SWIFT_PASS=000000

#The NODE Object Disk for Swift. example:md126p4.
OBJECT_DISK=sdb2

#The NODE IP for Swift Storage Network. example:x.x.x.x. ##计算节点
STORAGE_LOCAL_NET_IP=10.0.0.62

#--------------------Heat Config----------------------##
#Password for Mysql heat user. exmaple:000000
HEAT_DBPASS=000000

#Password for Keystore heat user. exmaple:000000
HEAT_PASS=000000

#--------------------Zun Config-----------------------##
#Password for Mysql Zun user. exmaple:000000
ZUN_DBPASS=000000

#Password for Keystore Zun user. exmaple:000000
ZUN_PASS=000000

#Password for Mysql Kuryr user. exmaple:000000
KURYR_DBPASS=000000

#Password for Keystore Kuryr user. exmaple:000000
KURYR_PASS=000000

#--------------------Ceilometer Config----------------##
#Password for Gnocchi ceilometer user. exmaple:000000
CEILOMETER_DBPASS=000000

#Password for Keystore ceilometer user. exmaple:000000
CEILOMETER_PASS=000000

#--------------------AODH Config----------------##
#Password for Mysql AODH user. exmaple:000000
AODH_DBPASS=000000

#Password for Keystore AODH user. exmaple:000000
AODH_PASS=000000

#--------------------Barbican Config----------------##
#Password for Mysql Barbican user. exmaple:000000
BARBICAN_DBPASS=000000

#Password for Keystore Barbican user. exmaple:000000
BARBICAN_PASS=000000


compute
[compute]
将从控制节点传输过来的openrc.sh配置文件进行修改，修改内容如下。
#Tunnel Network Interface. example:x.x.x.x
INTERFACE_IP=10.0.0.62
```

**ontroller节点和Compute节点**

### **执行脚本iaas-pre-host.sh进行安装**

```
【controller】
[root@controller ~]#iaas-pre-host.sh
【compute】
[root@compute ~]#iaas-pre-host.sh
```

**# 安装完成后同时重启**

```
reboot
```

 

### 数据库安装

在 controller 节点上使用 iaas-install-mysql.sh 脚本安装 Mariadb、Memcached、etcd 服务

```
【controller】
[root@controller ~]# iaas-install-mysql.sh

【compute】
[root@compute ~]# yum -y install MySQL-python
```

### Keystone 服务安装

在 controller 节点上使用 iaas-install-keystone.sh 脚本安装 Keystone 服务

```
【controller】
[root@controller ~]# iaas-install-keystone.sh
 source /etc/keystone/admin-openrc.sh
```

### Glance 安装

在 controller 节点上使用 iaas-install-glance.sh 脚本安装glance 服务

```
iaas-install-glance.sh
source /etc/keystone/admin-openrc.sh

##测试镜像上传情况
glance image-create --name "cirrors" --disk-format qcow2 --container-format bare --progress < cirrors.xxxx.img
glance image-list
openstack image list
```

### Nova 安装

在 controller节 点 和 compute节 点 上分别使用iaas-install-nova-controller.sh脚本iaas-install-nova-compute.sh 脚本安装 Nova 服务

```
【controller】
[root@controller ~]# iaas-install-nova-controller.sh
【compute】
[root@compute ~]# iaas-install-nova-compute.sh
```

### Neutron 安装

在 controller 节点和 compute节点上分别修改iaas-install-neutron-controller.sh脚本 、iaas-install-neutron-compute.sh 脚本分别安装Neutron 服务,执行完脚本后网络默认是vlan模式

```
【controller】
[root@controller ~]# iaas-install-neutron-controller.sh

【compute】
[root@compute ~]# iaas-install-neutron-compute.sh
!!! 使用真实服务器搭建的话，网络模式选择vlan模式；使用虚拟机搭建的话，网络模式选择flat模式。
```



### Doshboard 安装

在controller节点上使用iaas-install-dashboard.sh脚本安装dashboad服务

```
【controller】
[root@controller ~]# iaas-install-dashboard.sh
```

### Cinder 安装

在 控 制 节 点 和 计 算节点上分别使用iaas-install-cinder-controller.sh.sh 脚本和iaas-install-cinder-compute.sh 安装 cinder 服务。

```
【controler】
[root@controller ~]# iaas-install-cinder-controller.sh

【compute】
[root@compute ~]# iaas-install-cinder-compute.sh
```

### 安装Swift服务

```
【controller】
[root@controller ~]# iaas-install-swift-controller.sh

【compute】
[root@compute ~]# iaas-install-swift-compute.sh
```

### 安装heat服务

```
【controller】
[root@controller ~]# iaas-install-heat.sh
```

### 安装Zun服务

```
【controller】
[root@controller ~]# iaas-install-zun-controller.sh

【compute】
[root@compute ~]# iaas-install-zun-compute.sh
```

### 安装Ceilometer服务

```
【controller】
[root@controller ~]# iaas-install-ceilometer-controller.sh

【compute】
[root@compute~]# iaas-install-ceilometer-compute.sh
```

### 安装Aodh服务

```
【controller】
[root@controller ~]# iaas-install-aodh.sh
```

### 将控制节点资源到云平台

```
【controller】
修改openrc.sh
把compute节点的IP地址和主机名改为controller节点的IP和主机名
vi /etc/xiandian/openrc.sh
#Compute Node Manager IP. example:x.x.x.x
HOST_IP_NODE=192.168.1.10

#Compute Node hostname. example:compute
HOST_NAME_NODE=controller

在控制节点运行iaas-install-nova-compute.sh
[root@controller ~]# iaas-install-nova-compute.sh
执行过程中需要确认登录controller节点和输入controller节点root用户密码。
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '10.0.0.61' (ECDSA) to the list of known hosts.
root@10.0.0.61's password: 
+----+--------------+------------+------+---------+-------+----------------------------+
| ID | Binary       | Host       | Zone | Status  | State | Updated At                 |
+----+--------------+------------+------+---------+-------+----------------------------+
| 10 | nova-compute | compute    | nova | enabled | up    | 2020-11-03T07:34:09.000000 |
| 11 | nova-compute | controller | nova | enabled | up    | 2020-11-03T07:34:09.000000 |
+----+--------------+------------+------+---------+-------+----------------------------+
```

## 三、OpenStack 运维任务

### 镜像管理

在openstack私有云平台上，基于cirros-0.3.4-x86_64-disk.img 镜 像 ，使 用 命 令 创 建 一 个 名 为cirros 的镜像

```
glance image-create --name "cirros" --disk-format qcow2 --container-format bare</root/cirros-0.3.4-x86_64-disk.img
openstack image create "centos7" --file CentOS-7-x86_64-DVD-1804.iso --disk-format iso --container-format bare --public
```

### 实例类型管理

在 openstack 私有云平台上,使用命令创建一个名为 Fmin，ID为 1，内存为 1024 MB，磁盘为 10 GB，vcpu 数量为 1 的云主机类型

nova help flavor-create

nova flavor-create <name> <id> <ram> <disk> <vcpus>

```
[root@controller ~]# nova flavor-create Fmin 1 1024 10 1
或者
openstack flavor create --id 1 --ram 1024 --disk 10 --vcpus 1 Fmin
+----+------+-----------+------+-----------+------+-------+-------------+-----------+-------------+
| ID | Name | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public | Description |
+----+------+-----------+------+-----------+------+-------+-------------+-----------+-------------+
| 1  | Fmin | 1024      | 10   | 0         |      | 1     | 1.0         | True      | -           |
+----+------+-----------+------+-----------+------+-------+-------------+-----------+-------------+
```

### 创建云主机外部网络

创建一个名为extnet的外部网络

```
openstack help network create
openstack network create --project admin --provider-network-type flat --provider-physical-network provider --external  extnet
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | UP                                   |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2021-04-21T12:42:13Z                 |
| description               |                                      |
| dns_domain                | None                                 |
| id                        | 307272f6-fd12-475d-b184-4b7ac05a3c18 |
| ipv4_address_scope        | None                                 |
| ipv6_address_scope        | None                                 |
| is_default                | False                                |
| is_vlan_transparent       | None                                 |
| mtu                       | 1500                                 |
| name                      | extnet                               |
| port_security_enabled     | True                                 |
| project_id                | 5210e597a7e0492497ce724327e97795     |
| provider:network_type     | flat                                 |
| provider:physical_network | provider                             |
| provider:segmentation_id  | None                                 |
| qos_policy_id             | None                                 |
| revision_number           | 5                                    |
| router:external           | External                             |
| segments                  | None                                 |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| updated_at                | 2021-04-21T12:42:13Z                 |
+---------------------------+--------------------------------------+
```

#### 创建外部子网

```
openstack subnet create --network extnet --subnet-range 10.0.0.0/24 --gateway 10.0.0.254 --allocation-pool start=10.0.0.80,end=10.0.0.90 --dhcp extsubnet ##创建一个外部网络，相当于172的网段映射到10网段，我的机器现在都是10段的
+-------------------+--------------------------------------+
| Field             | Value                                |
+-------------------+--------------------------------------+
| allocation_pools  | 10.0.0.65-10.0.0.70                  |
| cidr              | 10.0.0.0/24                          |
| created_at        | 2021-04-21T12:44:08Z                 |
| description       |                                      |
| dns_nameservers   |                                      |
| enable_dhcp       | True                                 |
| gateway_ip        | 10.0.0.254                           |
| host_routes       |                                      |
| id                | bdd4f503-5fd1-4d66-8a7c-b8ecceb47621 |
| ip_version        | 4                                    |
| ipv6_address_mode | None                                 |
| ipv6_ra_mode      | None                                 |
| name              | extsubnet                            |
| network_id        | 307272f6-fd12-475d-b184-4b7ac05a3c18 |
| project_id        | 5210e597a7e0492497ce724327e97795     |
| revision_number   | 0                                    |
| segment_id        | None                                 |
| service_types     |                                      |
| subnetpool_id     | None                                 |
| tags              |                                      |
| updated_at        | 2021-04-21T12:44:08Z                 |
+-------------------+--------------------------------------+
```

### 创建云主机内部网络

```
openstack network create --internal intnet
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | UP                                   |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2021-04-21T12:44:53Z                 |
| description               |                                      |
| dns_domain                | None                                 |
| id                        | ab1c569d-03b6-435a-acce-a8d8ba4f1237 |
| ipv4_address_scope        | None                                 |
| ipv6_address_scope        | None                                 |
| is_default                | False                                |
| is_vlan_transparent       | None                                 |
| mtu                       | 1450                                 |
| name                      | intnet                               |
| port_security_enabled     | True                                 |
| project_id                | 5210e597a7e0492497ce724327e97795     |
| provider:network_type     | vxlan                                |
| provider:physical_network | None                                 |
| provider:segmentation_id  | 15                                   |
| qos_policy_id             | None                                 |
| revision_number           | 2                                    |
| router:external           | Internal                             |
| segments                  | None                                 |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| updated_at                | 2021-04-21T12:44:53Z                 |
+---------------------------+--------------------------------------+
```

#### 创建内部之网

```
openstack subnet create --network intnet --subnet-range 172.129.29.0/24 --gateway 172.129.29.1 --dhcp insubnet
+-------------------+--------------------------------------+
| Field             | Value                                |
+-------------------+--------------------------------------+
| allocation_pools  | 172.129.29.2-172.129.29.254          |
| cidr              | 172.129.29.0/24                      |
| created_at        | 2021-04-21T12:46:16Z                 |
| description       |                                      |
| dns_nameservers   |                                      |
| enable_dhcp       | True                                 |
| gateway_ip        | 172.129.29.1                         |
| host_routes       |                                      |
| id                | cb152c24-39ae-4933-a688-e0e665f61cdb |
| ip_version        | 4                                    |
| ipv6_address_mode | None                                 |
| ipv6_ra_mode      | None                                 |
| name              | insubnet                             |
| network_id        | ab1c569d-03b6-435a-acce-a8d8ba4f1237 |
| project_id        | 5210e597a7e0492497ce724327e97795     |
| revision_number   | 0                                    |
| segment_id        | None                                 |
| service_types     |                                      |
| subnetpool_id     | None                                 |
| tags              |                                      |
| updated_at        | 2021-04-21T12:46:16Z                 |
+-------------------+--------------------------------------+
```

### 添加路由

```
openstack router create ext-router
+-------------------------+--------------------------------------+
| Field                   | Value                                |
+-------------------------+--------------------------------------+
| admin_state_up          | UP                                   |
| availability_zone_hints |                                      |
| availability_zones      |                                      |
| created_at              | 2021-04-21T12:47:06Z                 |
| description             |                                      |
| distributed             | False                                |
| external_gateway_info   | None                                 |
| flavor_id               | None                                 |
| ha                      | False                                |
| id                      | 96470fa8-afcd-4af8-85db-c4e5d7f48b48 |
| name                    | ext-router                           |
| project_id              | 5210e597a7e0492497ce724327e97795     |
| revision_number         | 1                                    |
| routes                  |                                      |
| status                  | ACTIVE                               |
| tags                    |                                      |
| updated_at              | 2021-04-21T12:47:06Z                 |
+-------------------------+--------------------------------------+

openstack router set  --enable --enable-snat --external-gateway extnet ext-router  
openstack router add subnet ext-router insubnet  
```

### 修改默认安全组

```
查看安全组
openstack security group list
#创建规则
openstack security group rule create --ingress --protocol tcp 默认安全组ID
openstack security group rule create --ingress --protocol udp 默认安全组ID
openstack security group rule create --ingress --protocol icmp 默认安全组ID
openstack security group rule create --egress --protocol tcp 默认安全组ID
openstack security group rule create --egress --protocol udp 默认安全组ID
openstack security group rule create --egress --protocol icmp 默认安全组ID

openstack security group list
```



### 云主机管理

在 openstack 私有云平台上，基于“cirros”镜像、flavor 使用“Fmin”、extnet 的网络，创建一台虚拟机 VM1，启动 VM1，并使用 PC 机能远程登录到 VM1

```
nova boot --image cirrors --flavor Fmin --nic net-name=intnet VM1  ###这里是要用内部网络，现在172是内部网络，10是外部网络 
+--------------------------------------+------------------------------------------------+
| Property                             | Value                                          |
+--------------------------------------+------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                         |
| OS-EXT-AZ:availability_zone          |                                                |
| OS-EXT-SRV-ATTR:host                 | -                                              |
| OS-EXT-SRV-ATTR:hostname             | vm1                                            |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                                              |
| OS-EXT-SRV-ATTR:instance_name        |                                                |
| OS-EXT-SRV-ATTR:kernel_id            |                                                |
| OS-EXT-SRV-ATTR:launch_index         | 0                                              |
| OS-EXT-SRV-ATTR:ramdisk_id           |                                                |
| OS-EXT-SRV-ATTR:reservation_id       | r-7g0irp1e                                     |
| OS-EXT-SRV-ATTR:root_device_name     | -                                              |
| OS-EXT-SRV-ATTR:user_data            | -                                              |
| OS-EXT-STS:power_state               | 0                                              |
| OS-EXT-STS:task_state                | scheduling                                     |
| OS-EXT-STS:vm_state                  | building                                       |
| OS-SRV-USG:launched_at               | -                                              |
| OS-SRV-USG:terminated_at             | -                                              |
| accessIPv4                           |                                                |
| accessIPv6                           |                                                |
| adminPass                            | 7RkQD2xoYUWa                                   |
| config_drive                         |                                                |
| created                              | 2021-04-21T12:49:18Z                           |
| description                          | -                                              |
| flavor:disk                          | 10                                             |
| flavor:ephemeral                     | 0                                              |
| flavor:extra_specs                   | {}                                             |
| flavor:original_name                 | Fmin                                           |
| flavor:ram                           | 1024                                           |
| flavor:swap                          | 0                                              |
| flavor:vcpus                         | 1                                              |
| hostId                               |                                                |
| host_status                          |                                                |
| id                                   | 78bd9f35-08e5-47ce-99aa-d7f1d3c1cb93           |
| image                                | cirrors (ba2a5c2d-73ed-409a-9570-b4dc3441d30e) |
| key_name                             | -                                              |
| locked                               | False                                          |
| metadata                             | {}                                             |
| name                                 | VM1                                            |
| os-extended-volumes:volumes_attached | []                                             |
| progress                             | 0                                              |
| security_groups                      | default                                        |
| status                               | BUILD                                          |
| tags                                 | []                                             |
| tenant_id                            | 5210e597a7e0492497ce724327e97795               |
| updated                              | 2021-04-21T12:49:18Z                           |
| user_id                              | 444dca8f058d4859a55755d2495226be               |
+--------------------------------------+------------------------------------------------+
```

### 创建浮动IP地址

```
neutron floatingip-create extnet
neutron CLI is deprecated and will be removed in the future. Use openstack CLI instead.
Created a new floatingip:
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| created_at          | 2021-04-21T12:50:08Z                 |
| description         |                                      |
| fixed_ip_address    |                                      |
| floating_ip_address | 10.0.0.70                            |
| floating_network_id | 307272f6-fd12-475d-b184-4b7ac05a3c18 |
| id                  | 1e45ee48-5e67-437f-b774-a32e7bc25547 |
| port_id             |                                      |
| project_id          | 5210e597a7e0492497ce724327e97795     |
| revision_number     | 0                                    |
| router_id           |                                      |
| status              | DOWN                                 |
| tags                |                                      |
| tenant_id           | 5210e597a7e0492497ce724327e97795     |
| updated_at          | 2021-04-21T12:50:08Z                 |
+---------------------+--------------------------------------+
neutron floatingip-list
neutron CLI is deprecated and will be removed in the future. Use openstack CLI instead.
+--------------------------------------+----------------------------------+------------------+---------------------+---------+
| id                                   | tenant_id                        | fixed_ip_address | floating_ip_address | port_id |
+--------------------------------------+----------------------------------+------------------+---------------------+---------+
| 1e45ee48-5e67-437f-b774-a32e7bc25547 | 5210e597a7e0492497ce724327e97795 |                  | 10.0.0.70           |         |
+--------------------------------------+----------------------------------+------------------+---------------------+---------+
```

### 绑定浮动IP地址

```
struct sysinfo s_info;
int error = sysinfo(&s_info);
if(s_info.totalram>1*1024*1024*1024)
{
   logv("error0: %d, total: %lu free: %lu \n", error, s_info.totalram, s_info.freeram);
}
neutron floatingip-list （查看浮动ipid）
neutron port-list （查看端口id）
neutron floatingip-associate --fixed-ip-address 浮动IP 浮动ipid 端口id
neutron floatingip-associate --fixed-ip-address 172.129.29.9 1e45ee48-5e67-437f-b774-a32e7bc25547 ebe16451-0ab3-4ee4-a570-183349c905d0


ssh 10.0.0.76 -l 登录名字
```

![image-20210422195850445](C:\Users\luo\AppData\Roaming\Typora\typora-user-images\image-20210422195850445.png)

![image-20210422195938633](C:\Users\luo\AppData\Roaming\Typora\typora-user-images\image-20210422195938633.png)

### 创建卷lvm

在 openstack 私有云平台上，创建一个名为“lvm”的卷类型，创建一块卷设备，名字为 block、类型为 lvm 的 40G 云硬盘，并附加到虚拟机 VM1上

```
openstack volume type create lvm
+-------------+--------------------------------------+
| Field       | Value                                |
+-------------+--------------------------------------+
| description | None                                 |
| id          | f0743aa8-f0bc-4e50-a619-ba67d40d404e |
| is_public   | True                                 |
| name        | lvm                                  |
+-------------+--------------------------------------+
openstack volume create --type lvm --size 10 block
+---------------------+--------------------------------------+
| attachments         | []                                   |
| availability_zone   | nova                                 |
| bootable            | false                                |
| consistencygroup_id | None                                 |
| created_at          | 2021-04-29T06:16:58.000000           |
| description         | None                                 |
| encrypted           | False                                |
| id                  | a62e711c-1679-4081-9dda-f1413b49667d |
| migration_status    | None                                 |
| multiattach         | False                                |
| name                | bolck                                |
| properties          |                                      |
| replication_status  | None                                 |
| size                | 10                                   |
| snapshot_id         | None                                 |
| source_volid        | None                                 |
| status              | creating                             |
| type                | lvm                                  |
| updated_at          | None                                 |
| user_id             | d51e8ef9364c4d3283fe79089122a957     |
+---------------------+--------------------------------------+

nova list #查看虚拟机状态
nova卷挂载
openstack image list  
openstack volume create --image 镜像ID --size 20 --availability-zone nova 卷名字
nova list
openstack server add volume 实例ID 卷ID --device /dev/vdb 
```

![image-20210422211300525](C:\Users\luo\AppData\Roaming\Typora\typora-user-images\image-20210422211300525.png)

### Raid 管理

在 OpenStack 私有云平台，创建一台云主机，并创建一个40G 大小的cinder块存储,将块存储连接到云主机，然后在云主机上对云 硬盘进行操作。要求分出 4 个大小为 5G 的分区，使用这 4 个分区，创建名为/dev/md5、raid 级别为5的磁盘阵列加一个热备盘（使用 最后一个分区作为热备盘）

```
mdadm -Cv /dev/md5 -l 5 -n 3 /dev/vdb1 /dev/vdb2 /dev/vdb3 --spare-devices=1 /dev/vdb4 或者
mdadm --create /dev/md5 -a yes -l 5 -n 3 /dev/sdb1 /dev/sdb2 /dev/sdb3 --spare-devices=1 /dev/vdb4
ls -l /dev/md5
mdadm -D --scan >/etc/mdadm.conf
cat /etc/mdadm.conf
mdadm --misc --detail /dev/md5
```

### 编写/root/openstack/deletevm.sh脚本，完成释放虚拟机VM1

```
openstack server shalve SERVERNAME ###释放实例虚拟机VM1
nova shelve-offload SERVERNAME     ###将释放的实例彻底删除
```

### 脚本改内存大小

在Controller节点中编写名为modvm.sh的。shell脚本查看云主机VM1的内存大小，如果内存小于2G，调整云主机VM1的内存为2G

```
#!/bin/bash 
total=$(nova show VM1 |grep flavor:ram |sed 's/[^0-9]//g') 
echo $total 
if [ $total -lt 2048 ] 
then 
nova flavor-create Fmin3 3 2048 20 1 
sed -i 's/\[DEFAULT\]/\[DEFAULT\]\nallow_resize_to_same_host=true\nresize_confirm_window=5/g' /etc/nova/nova.conf
sed -i 's/\[filter_scheduler\]/\[filter_scheduler\]\nscheduler_default_filters=AllHostsFilter/g' /etc/nova/nova.conf .
n
openstack server resize --flavor 3 VM1 
#openstack server resize --confirm VM1 
nova show VM1 
fi





kubectl -n kubernetes-dashboard get secret | grep dashboard-admin | awk '{print $1}'
```

### 编写server.yaml 创建云主机类型.

在openstack私有云平台上，编写模板server.yml，创建名为“m1.flavor”、 ID 为 1234、 内存为 1024MB、硬盘为 20GB、 vcpu数量为 2的云主机类型

```
heat_template_version: 2018-03-02
resources: 
  server: 
    type: OS::Nova::Flavor 
    properties: 
      disk: 20
      flavorid: 1234
      ram: 1024
      cpu: 1
      name: m1.flavor
      
      
openstack stack create -t server.yaml  server
```



### 创建快照

在openstack私有云平台上，将云主机VM1保存为qcow2格式的快照并保存到controller节点/root/cloudsave目录下

```
mkdir cloudsave
openstack server image create VM1 --name tt
openstack image save --file /root/cloudsave/tt.qcow2 tt
```

### 分区格式化，永久挂载

在云主机上对云盘进行分区格式化，并且永久挂载到/opt目录下

```
##分区
fdisk /dev/vdb
m
n
e
数字
回车
回车
##格式化分区
mkfs -t ext3 /dev/vdb
##永久挂载
vi /etc/fstab
/dev/vdb /opt ext3 defaults 1 1
```

### swift后端存储

```
[root@controller ~]# cat /etc/glance/glance-api.conf |egrep -v  '^$|^#'
[glance_store]
stores =  glance.store.swift.Store
default_store = swift
swift_store_region = RegionOne
swift_store_endpoint_type = internalURL
swift_store_container = glance
swift_store_large_object_size = 5120
swift_store_large_object_chunk_size = 200
swift_store_create_container_on_put = True
swift_store_multi_tenant = True
swift_store_admin_tenants = service
swift_store_auth_address = http://controller:5000/v3.0/
swift_store_user = glance
swift_store_key = 000000
```

### 数据库主从管理

使用 OpenStack 私有云平台，创建两台云主机 vm1 和 vm2，在这两台云主机上分别安装数据库服务，并配置成主从数据库，vm1 节点为主库，vm2 节点为从库（数据库密码设置为 000000）

```
##双节点修改主机名
##双节点关闭防火墙及SELinux服务
systemctl stop firewalld
##双节点配置hosts
##双节点配置YUM
tar -zxvf mariadb-repo.tar.gz -C /opt/
vi /etc/yum.repos.d/local.repo 
[mariadb]
name=mariadb
baseurl=file:///opt/mariadb-repo
gpgcheck=0
enabled=1
##双节点安装数据库服务，并且设置启动和开机自启
yum install -y mariadb mariadb-server
systemctl start mariadb
systemctl enable mariadb
##双节点初始化数据库并配置主从服务
mysql_secure_installation
回车
y
000000
000000
y
n
y
y
##配置mysql1主节点
vi /etc/my.cnf.d/server.cnf
[mysqld]
log_bin = mysql-bin       #记录操作日志
binlog_ignore_db = mysql  #不同步mysql系统数据库
server_id = 12            #数据库集群中的每个节点id都要不同，一般使用IP地址的最后段的数字，例172.30.11.12，server_id就写12

##重启数据库，并且进入数据库
systemctl restart mariadb
mysql -uroot -p000000
##在主节点赋予任何机器登录mysql，然后在主节点上创建一个user用户连接节点mysql2，并赋予从节点同步主节点数据库的权限
grant all privileges  on *.* to root@'%' identified by "000000";
grant replication slave on *.* to 'user'@'mysql2' identified by '000000';
flush privileges;
##配置mysql2从节点并且重启
vi /etc/my.cnf.d/server.cnf
[mysqld]
log_bin = mysql-bin       #记录操作日志
binlog_ignore_db = mysql  #不同步mysql系统数据库
server_id = 12            #数据库集群中的每个节点id都要不同，一般使用IP地址的最后段的数字，例172.30.11.12，server_id就写12
systemctl restart mariadb
##配置从节点连接信息master_host为主节点主机名mysql1，master_user为上一步中创建的用户user
mysql -uroot -p000000
change master to master_host='mysql1',master_user='user',master_password='000000';

##开启节点服务，用show slave status\G命令，并查看从节点服务状态，如果Slave_IO_Running和Slave_SQL_Running的状态都为YES，则从节点服务开启成功
start slave;
show slave status\G 

##验证数据库主从服务,先在主节点mysql1中创建库test，并在库test中创建表company，插入表数据，创建完成后，查看表company数据
mysql -uroot -p000000
create database test;
use test;
create table company(id int not null primary key,name varchar(50),addr varchar(255));
insert into company values(1,"alibaba","china");
select * from company;

##从节点验证复制功能,登录mysql2节点的数据库，查看数据库列表。找到test数据库，查询表，并查询内容验证从数据库的复制功能
show databases;
use test;
show tables;
select * from company;
```

### 数据库加速读写分离

| **IP**    | **主机名** | **节点**         |
| --------- | ---------- | ---------------- |
| 10.0.0.64 | mysql1     | 主数据库节点     |
| 10.0.0.65 | mysql2     | 从数据库节点     |
| 10.0.0.66 | mycat      | 数据库中间件节点 |

#### 安装JDK环境

```
在配置好本地yum源后
[root@mycat ~]# yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-devel
查看安装好的JDK
[root@mycat ~]# java -version
```

#### Mycat 读写分离中间件服务

```
将二进制软件包解压到/usr/local目录，并赋予目录权限
[root@mycat ~]# tar -zxvf Mycat-server-1.6-RELEASE-20161028204710-linux.tar.gz -C /usr/local/
[root@mycat ~]# chmod -R 777 /usr/local/mycat/
```

#### 增加环境变量

```
[root@mycat ~]# echo export MYCAT_HOME=/usr/local/mycat/ >> /etc/profile
[root@mycat ~]# source /etc/profile
```



#### 编辑配置文件

```
将原来的配置清除sc	
[root@mycat ~]# vi /usr/local/mycat/conf/schema.xml 
<?xml version="1.0"?>
<!DOCTYPE mycat:schema SYSTEM "schema.dtd">
<mycat:schema xmlns:mycat="http://io.mycat/">
<schema name="USERDB" checkSQLschema="true" sqlMaxLimit="100" dataNode="dn1"></schema> 
<dataNode name="dn1" dataHost="localhost1" database="test" />  
<dataHost name="localhost1" maxCon="1000" minCon="10" balance="3" dbType="mysql" dbDriver="native" writeType="0" switchType="1"  slaveThreshold="100">  
    <heartbeat>select user()</heartbeat>
    <writeHost host="hostM1" url="10.0.0.64:3306" user="root" password="000000"> ##mysql1主节点1
        <readHost host="hostS1" url="10.0.0.65:3306" user="root" password="000000" />  ##mysql2从节点
    </writeHost>
</dataHost>
</mycat:schema>
```

#### 修改配置文件的权限

```
[root@mycat ~]# chown root:root /usr/local/mycat/conf/schema.xml
[root@mycat ~]# chmod -R 777 /usr/local/mycat/  ###由于上面的schema.xml文件是删除重新创建的之前赋予的权限就丢失了
```

#### 编辑mycat的访问用户

``` 
[root@mycat ~]# vi /usr/local/mycat/conf/server.xml 
在文件的最后部分修改
<user name="root">
		<property name="password">000000</property>
		<property name="schemas">USERDB</property>
</user>
删除
<user name="user">
		<property name="password">user</property>
		<property name="schemas">TESTDB</property>
		<property name="readOnly">true</property>
</user>
```

#### 启动并验证服务

```
[root@mycat ~]# /bin/bash /usr/local/mycat/bin/mycat start
用netstat -ntlp查看是否开放8066和9066端口，开启表示启动成功
```

#### 验证数据库集群服务读写

```
先安装数据库
[root@mycat ~]# yum install -y MariaDB-client
[root@mycat ~]# mysql -h127.0.0.1 -P8066 -uroot -p000000
use USERDB;
show tables;
这里应该能看到前面主从创建的数据表
insert into company values(2,"bastetball","usa"); ##增加一条数据测试
通过9066端口查询数据库读写操作信息，写是主节点，读是从节点
[root@mycat ~]# mysql -h127.0.0.1 -P9066 -uroot -p000000 -e 'show @@datasource';
+----------+--------+-------+-----------+------+------+--------+------+------+---------+-----------+------------+
| DATANODE | NAME   | TYPE  | HOST      | PORT | W/R  | ACTIVE | IDLE | SIZE | EXECUTE | READ_LOAD | WRITE_LOAD |
+----------+--------+-------+-----------+------+------+--------+------+------+---------+-----------+------------+
| dn1      | hostM1 | mysql | 10.0.0.64 | 3306 | W    |      0 |   10 | 1000 |      59 |         3 |          1 |
| dn1      | hostS1 | mysql | 10.0.0.65 | 3306 | R    |      0 |    0 | 1000 |       0 |         0 |          0 |
+----------+--------+-------+-----------+------+------+--------+------+------+---------+-----------+------------+
```



### 云平台安全策略提升

使用 OpenStack 私有云平台，通过提供的相关软件包，安装必要组件，将私有云平台的访问策略从 http 提升至 https。

| **IP**    | **主机名** | **节点**          |
| --------- | ---------- | ----------------- |
| 10.0.0.61 | controller | OpenStack控制节点 |
| 10.0.0.62 | compute    | OpenStack计算节点 |

#### 配置yum源

```
tar -zxvf https-repo.tar.gz -C /opt
vi /etc/yum.repos.d/ssl.repo
[ssl]
name=ssl
baseurl=file:///opt/https-repo
gpgcheck=0
enabled=1
```

#### 安装服务

```
yum install -y mod_wsgi httpd mod_ssl
```

#### 修改ssl配置文件

```
vi /etc/httpd/conf.d/ssl.conf
SSLProtocol all -SSLv2 -SSLv3    //找到该行，并注释
SSLProtocol all -SSLv2            //添加该行  
```

#### 开启SSL

```
vi /etc/openstack-dashboard/local_settings
CSRF_COOKIE_SECURE = True             //将该行的注释取消
SESSION_COOKIE_SECURE = True          //将该行的注释取消
USE_SSL = True                           //添加该行
SESSION_COOKIE_HTTPONLY = True       //添加该行
```

#### 重启服务并且访问dashboard

```
service httpd restart
service memcached restart
https://10.0.0.61/dashboard
```

### Openstack手动迁移虚拟机

```
在项目→资源管理器→云主机→云主机名称查看此台云主机归属哪个物理机上或者使用命令
nova show test

## 执行手动迁移虚拟机的过程有2步，需要将虚拟机实例目录进行转移，并修改数据库文件内容
##第1步
##1.1，找到虚拟机存放3位置，一般存放在/var/lib/nova/instances/
cd /var/lib/nova/instances/
##1.2,迁移虚拟机目录
scp -r 虚拟机ID root@哪台机器:/var/lib/nova/instances/
##1.3，拷贝完成后删除原来的虚拟机
rm -rf 虚拟机ID
##1.4，去拷贝完成的虚拟机修改用户和用户组，都改成nova
chown nova:nova 虚拟机ID  

##第2步
##2.1,修改数据库文件
mysql -uroot -p000000
use nova;
show tables;
##2.2,将instances表中test虚拟机的host和node字段都改成controller
update instances set host='controller', node='controller' where uuid='虚拟机ID';
##2.3，重启nova-compute服务，并且开启虚拟机
systemctl restart  openstack-nova-compute
nova start 虚拟机ID
nova list
```

### Rabbitmq集群

#### 规划节点

| **IP**    | **主机名** | **节点**          |
| --------- | ---------- | ----------------- |
| 10.0.0.67 | rabbitmq1  | RabbitMQ 磁盘节点 |
| 10.0.0.68 | rabbitmq2  | RabbitMQ 内存节点 |
| 10.0.0.70 | rabbitmq3  | RabbitMQ 内存节点 |

#### 基础准备

- 三个节点的YUM
- 三个节点的hosts
- 三个节点的selinux
- 三个节点的名字
- 三个节点的防火墙

#### 三个节点安装Rabbitmq

```
yum install -y rabbitmq-server
systemctl start rabbitmq-server
systemctl status rabbitmq-server
```

#### Rabbitmq1配置磁盘节点

```
[root@rabbitmq1 ~]# rabbitmq-plugins enable rabbitmq_management
[root@rabbitmq1 ~]# service rabbitmq-server restart
http://IP地址:15672
```

#### 配置集群服务

```
将rabbitmq1的.erlang.cookie文件，并将该文件复制到rabbitmq2和rabbitmq3节点的/var/lib/rabbitmq/
[root@rabbitmq1 ~]# scp /var/lib/rabbitmq/.erlang.cookie root@rabbitmq2:/var/lib/rabbitmq/
[root@rabbitmq1 ~]# scp /var/lib/rabbitmq/.erlang.cookie root@rabbitmq3:/var/lib/rabbitmq/
[root@rabbitmq2 rabbitmq]# chown rabbitmq:rabbitmq .erlang.cookie
[root@rabbitmq3 rabbitmq]# chown rabbitmq:rabbitmq .erlang.cookie
```

#### 配置节点加入集群

```
[root@rabbitmq2 rabbitmq]# rabbitmqctl stop_app
[root@rabbitmq2 rabbitmq]# rabbitmqctl join_cluster --ram rabbit@rabbitmq1
[root@rabbitmq3 rabbitmq]# rabbitmqctl start_app
[root@rabbitmq2 rabbitmq]# rabbitmq-plugins enable rabbitmq_management
[root@rabbitmq2 ~]# service rabbitmq-server restart

[root@rabbitmq3 rabbitmq]# rabbitmqctl stop_app
[root@rabbitmq3 rabbitmq]# rabbitmqctl join_cluster --ram rabbit@rabbitmq1
[root@rabbitmq3 rabbitmq]# rabbitmqctl start_app
[root@rabbitmq3 rabbitmq]# rabbitmq-plugins enable rabbitmq_management
[root@rabbitmq3 ~]# service rabbitmq-server restart

####如果要使rabbitmq2、rabbitmq3都是磁盘节点，去掉--ram参数即可
```

![image-20210425202948803](C:\Users\luo\AppData\Roaming\Typora\typora-user-images\image-20210425202948803.png)

### Swift分片

```
## 查询swift状态
swift stat
##查看swift存储区域
cd /etc/swift
swift-ring-builder object.builder 



##创建容器
swift post 容器名称
##上传镜像进行分片存储
swift upload 容器名称 -S 10000000 镜像位置
##查看存储路径中的数据片
swift list test_segments
```

 